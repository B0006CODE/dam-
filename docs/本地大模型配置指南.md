# æœ¬åœ°å¤§æ¨¡å‹é…ç½®æŒ‡å—

æœ¬é¡¹ç›®æ”¯æŒä½¿ç”¨æœ¬åœ°å¤§æ¨¡å‹ï¼Œæ— éœ€ä¾èµ–äº‘æœåŠ¡ APIã€‚ç›®å‰æ”¯æŒä¸¤ç§ä¸»æµçš„æœ¬åœ°éƒ¨ç½²æ–¹å¼ï¼š

## ğŸ“¦ æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨ Ollamaï¼ˆæ¨èï¼‰

### é€‚ç”¨åœºæ™¯
- ä¸ªäººå¼€å‘å’Œæµ‹è¯•
- å¿«é€ŸåŸå‹å¼€å‘
- ä¸éœ€è¦é«˜å¹¶å‘çš„åœºæ™¯
- å¸Œæœ›ç®€å•æ˜“ç”¨çš„éƒ¨ç½²æ–¹å¼

### é…ç½®æ­¥éª¤

#### 1. å®‰è£… Ollama

**Windows:**
- è®¿é—® https://ollama.com ä¸‹è½½å®‰è£…ç¨‹åº
- åŒå‡»å®‰è£…ï¼Œé»˜è®¤ä¼šåœ¨åå°è¿è¡Œ

**Linux/Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### 2. ä¸‹è½½æ¨¡å‹

```bash
# èŠå¤©æ¨¡å‹ï¼ˆé€‰æ‹©ä¸€ä¸ªï¼‰
ollama pull qwen2.5:7b          # æ¨èï¼š7B å‚æ•°ï¼Œæ€§èƒ½å‡è¡¡
ollama pull qwen2.5:14b         # 14B å‚æ•°ï¼Œæ•ˆæœæ›´å¥½ä½†éœ€è¦æ›´å¤šèµ„æº
ollama pull llama3.1:8b         # Llama 3.1 8B
ollama pull deepseek-r1:7b      # DeepSeek R1 7B

# Embedding æ¨¡å‹ï¼ˆå¿…éœ€ï¼‰
ollama pull nomic-embed-text    # è‹±æ–‡é¦–é€‰
ollama pull bge-m3              # ä¸­è‹±æ–‡æ··åˆ
```

#### 3. é…ç½®é¡¹ç›®

Ollama é…ç½®å·²ç»æ·»åŠ åˆ° `src/config/static/models.yaml` ä¸­ï¼Œæ‚¨åªéœ€è¦ï¼š

**åœ¨ç³»ç»Ÿè®¾ç½®ç•Œé¢ä¸­ï¼š**
1. é€‰æ‹©é»˜è®¤å¯¹è¯æ¨¡å‹ï¼š`ollama/qwen2.5:7b`
2. é€‰æ‹© Embedding æ¨¡å‹ï¼š`ollama/bge-m3` æˆ– `ollama/nomic-embed-text`

**æˆ–è€…ä¿®æ”¹é…ç½®æ–‡ä»¶** `saves/config/base.yaml`ï¼š
```yaml
default_model: ollama/qwen2.5:7b
embed_model: ollama/bge-m3
```

#### 4. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
ollama list

# æµ‹è¯•æ¨¡å‹
ollama run qwen2.5:7b "ä½ å¥½"
```

---

## ğŸš€ æ–¹æ¡ˆäºŒï¼šä½¿ç”¨ vLLMï¼ˆé«˜æ€§èƒ½ï¼‰

### é€‚ç”¨åœºæ™¯
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- éœ€è¦é«˜å¹¶å‘å¤„ç†
- æœ‰ GPU èµ„æº
- éœ€è¦æ›´å¥½çš„æ¨ç†æ€§èƒ½

### é…ç½®æ­¥éª¤

#### 1. å®‰è£… vLLM

**ä½¿ç”¨ pip:**
```bash
pip install vllm
```

**ä½¿ç”¨ Dockerï¼ˆæ¨èï¼‰:**
```bash
docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model Qwen/Qwen2.5-7B-Instruct
```

#### 2. å¯åŠ¨ vLLM æœåŠ¡

**æœ¬åœ°å¯åŠ¨:**
```bash
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000
```

**æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„:**
```bash
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/your/local/model \
    --host 0.0.0.0 \
    --port 8000
```

#### 3. é…ç½®é¡¹ç›®

vLLM é…ç½®å·²ç»æ·»åŠ åˆ° `src/config/static/models.yaml` ä¸­ï¼Œæ‚¨åªéœ€è¦ï¼š

**åœ¨ç³»ç»Ÿè®¾ç½®ç•Œé¢ä¸­ï¼š**
1. é€‰æ‹©é»˜è®¤å¯¹è¯æ¨¡å‹ï¼š`vllm/Qwen/Qwen2.5-7B-Instruct`
2. é€‰æ‹© Embedding æ¨¡å‹ï¼š`vllm/Qwen/Qwen3-Embedding-0.6B`
3. å¦‚éœ€é‡æ’åºï¼Œé€‰æ‹©ï¼š`vllm/BAAI/bge-reranker-v2-m3`

**æˆ–è€…ä¿®æ”¹é…ç½®æ–‡ä»¶** `saves/config/base.yaml`ï¼š
```yaml
default_model: vllm/Qwen/Qwen2.5-7B-Instruct
embed_model: vllm/Qwen/Qwen3-Embedding-0.6B
reranker: vllm/BAAI/bge-reranker-v2-m3
enable_reranker: true
```

#### 4. éªŒè¯å®‰è£…

```bash
# æµ‹è¯• API æ˜¯å¦æ­£å¸¸
curl http://localhost:8000/v1/models

# æµ‹è¯•èŠå¤©
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "messages": [{"role": "user", "content": "ä½ å¥½"}]
  }'
```

---

## ğŸ”§ è¿›é˜¶é…ç½®

### ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶

å¦‚æœæ‚¨å·²ç»ä¸‹è½½äº†æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œå¯ä»¥é€šè¿‡è®¾ç½® `MODEL_DIR` ç¯å¢ƒå˜é‡æ¥æŒ‡å®šæ¨¡å‹ç›®å½•ï¼š

**åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ï¼š**
```bash
MODEL_DIR=/path/to/your/models
```

### Docker ç¯å¢ƒé…ç½®

å¦‚æœåœ¨ Docker ä¸­è¿è¡Œï¼Œéœ€è¦ç¡®ä¿å®¹å™¨èƒ½è®¿é—®ä¸»æœºçš„ Ollama æˆ– vLLM æœåŠ¡ï¼š

**æ–¹å¼ 1ï¼šä½¿ç”¨ host ç½‘ç»œæ¨¡å¼**
```yaml
# docker-compose.yml
services:
  api:
    network_mode: "host"
```

**æ–¹å¼ 2ï¼šä½¿ç”¨ host.docker.internal**
ä¿®æ”¹ `src/config/static/models.yaml` ä¸­çš„ base_urlï¼š
```yaml
ollama:
  base_url: http://host.docker.internal:11434/v1

vllm:
  base_url: http://host.docker.internal:8000/v1
```

### æ··åˆä½¿ç”¨äº‘æœåŠ¡å’Œæœ¬åœ°æ¨¡å‹

æ‚¨å¯ä»¥åŒæ—¶é…ç½®å¤šä¸ªæ¨¡å‹æä¾›å•†ï¼š
- å¿«é€Ÿå“åº”ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼š`ollama/qwen2.5:7b`
- å¤æ‚ä»»åŠ¡ä½¿ç”¨äº‘æœåŠ¡ï¼š`dashscope/qwen-max-latest`

åœ¨ç³»ç»Ÿè®¾ç½®ä¸­å¯ä»¥çµæ´»åˆ‡æ¢ä¸åŒçš„æ¨¡å‹ã€‚

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | å¯åŠ¨é€Ÿåº¦ | æ¨ç†é€Ÿåº¦ | å¹¶å‘èƒ½åŠ› | èµ„æºå ç”¨ | æ˜“ç”¨æ€§ |
|------|---------|---------|---------|---------|--------|
| Ollama | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| vLLM | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­ |

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: Ollama æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Ÿ
**A:** 
- æ£€æŸ¥ç«¯å£ 11434 æ˜¯å¦è¢«å ç”¨
- Windows å¯èƒ½éœ€è¦ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ
- æŸ¥çœ‹æ—¥å¿—ï¼š`ollama serve`

### Q2: vLLM æ˜¾å­˜ä¸è¶³ï¼Ÿ
**A:** 
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ 7B è€Œä¸æ˜¯ 14Bï¼‰
- å¯ç”¨é‡åŒ–ï¼š`--quantization awq` æˆ– `--quantization gptq`
- å‡å°‘ `--max-model-len` å‚æ•°

### Q3: Docker å®¹å™¨æ— æ³•è¿æ¥æœ¬åœ°æœåŠ¡ï¼Ÿ
**A:** 
- ç¡®ä¿ä½¿ç”¨ `host.docker.internal` è€Œä¸æ˜¯ `localhost`
- æˆ–ä½¿ç”¨ `--network host` æ¨¡å¼

### Q4: æ¨¡å‹ä¸‹è½½é€Ÿåº¦æ…¢ï¼Ÿ
**A:** 
- ä½¿ç”¨å›½å†…é•œåƒï¼š
  ```bash
  export HF_ENDPOINT=https://hf-mirror.com
  ```

### Q5: å¦‚ä½•æŸ¥çœ‹å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Ÿ
**A:**
- Ollama: `ollama list`
- vLLM: è®¿é—® `http://localhost:8000/v1/models`

---

## ğŸ“ æ¨èé…ç½®

### ä¸ªäººå¼€å‘ç¯å¢ƒï¼ˆ16GB RAMï¼‰
```yaml
default_model: ollama/qwen2.5:7b
embed_model: ollama/bge-m3
enable_reranker: false
```

### å›¢é˜Ÿç”Ÿäº§ç¯å¢ƒï¼ˆGPU æœåŠ¡å™¨ï¼‰
```yaml
default_model: vllm/Qwen/Qwen2.5-14B-Instruct
embed_model: vllm/Qwen/Qwen3-Embedding-0.6B
reranker: vllm/BAAI/bge-reranker-v2-m3
enable_reranker: true
```

### æ··åˆé…ç½®ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰
```yaml
default_model: ollama/qwen2.5:7b           # æ—¥å¸¸å¯¹è¯
fast_model: siliconflow/THUDM/GLM-4-9B-0414  # å¿«é€Ÿå“åº”ï¼ˆäº‘ç«¯ï¼‰
embed_model: ollama/bge-m3                  # Embedding
```

---

## ğŸ”— ç›¸å…³èµ„æº

- [Ollama å®˜ç½‘](https://ollama.com)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [Qwen æ¨¡å‹åº“](https://huggingface.co/Qwen)
- [Hugging Face é•œåƒ](https://hf-mirror.com)

---

å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·æŸ¥çœ‹é¡¹ç›® Issues æˆ–æäº¤æ–°çš„ Issueã€‚
