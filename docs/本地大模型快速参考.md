# ğŸš€ æœ¬åœ°å¤§æ¨¡å‹å¿«é€Ÿå‚è€ƒ

## ä¸€åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### Windows ç”¨æˆ·
```powershell
# 1. è¿è¡Œé…ç½®è„šæœ¬
.\scripts\setup-ollama.ps1

# 2. é€‰æ‹©æ¨¡å‹ï¼ˆæ¨è qwen2.5:7b + bge-m3ï¼‰

# 3. åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®ï¼š
#    é»˜è®¤å¯¹è¯æ¨¡å‹: ollama/qwen2.5:7b
#    Embedding æ¨¡å‹: ollama/bge-m3
```

### Linux/Mac ç”¨æˆ·
```bash
# 1. è¿è¡Œé…ç½®è„šæœ¬
chmod +x scripts/setup-ollama.sh
./scripts/setup-ollama.sh

# 2. é€‰æ‹©æ¨¡å‹ï¼ˆæ¨è qwen2.5:7b + bge-m3ï¼‰

# 3. åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®ï¼š
#    é»˜è®¤å¯¹è¯æ¨¡å‹: ollama/qwen2.5:7b
#    Embedding æ¨¡å‹: ollama/bge-m3
```

---

## ğŸ“‹ å¿«é€Ÿå‘½ä»¤

### Ollama å¸¸ç”¨å‘½ä»¤

```bash
# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list

# ä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:7b

# è¿è¡Œæ¨¡å‹æµ‹è¯•
ollama run qwen2.5:7b "ä½ å¥½"

# åˆ é™¤æ¨¡å‹
ollama rm qwen2.5:7b

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show qwen2.5:7b

# å¯åŠ¨æœåŠ¡ï¼ˆå¦‚æœæœªè‡ªåŠ¨å¯åŠ¨ï¼‰
ollama serve
```

### vLLM å¸¸ç”¨å‘½ä»¤

```bash
# å¯åŠ¨ vLLM æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --port 8000

# ä½¿ç”¨ Docker å¯åŠ¨
docker run --gpus all -p 8000:8000 \
    vllm/vllm-openai:latest \
    --model Qwen/Qwen2.5-7B-Instruct

# æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models

# æµ‹è¯• API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "Qwen/Qwen2.5-7B-Instruct", "messages": [{"role": "user", "content": "ä½ å¥½"}]}'
```

---

## ğŸ¯ æ¨èæ¨¡å‹ç»„åˆ

### ä¸ªäººå¼€å‘ï¼ˆ16GB RAMï¼‰
- **å¯¹è¯**: `ollama/qwen2.5:7b`
- **Embedding**: `ollama/bge-m3`
- **Reranker**: å…³é—­
- **æ˜¾å­˜éœ€æ±‚**: ~4GB

### å›¢é˜Ÿä½¿ç”¨ï¼ˆ32GB RAMï¼‰
- **å¯¹è¯**: `ollama/qwen2.5:14b`
- **Embedding**: `ollama/bge-m3`
- **Reranker**: å…³é—­
- **æ˜¾å­˜éœ€æ±‚**: ~8GB

### ç”Ÿäº§ç¯å¢ƒï¼ˆGPU æœåŠ¡å™¨ï¼‰
- **å¯¹è¯**: `vllm/Qwen/Qwen2.5-14B-Instruct`
- **Embedding**: `vllm/Qwen/Qwen3-Embedding-0.6B`
- **Reranker**: `vllm/BAAI/bge-reranker-v2-m3`
- **æ˜¾å­˜éœ€æ±‚**: ~12GB

---

## ğŸ”§ é…ç½®æ–‡ä»¶ä½ç½®

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `src/config/static/models.yaml` | å…¨å±€æ¨¡å‹åˆ—è¡¨é…ç½® |
| `saves/config/base.yaml` | è¿è¡Œæ—¶é…ç½®ï¼ˆä¿®æ”¹è¿™ä¸ªï¼‰|
| `examples/config-local-models.yaml` | é…ç½®ç¤ºä¾‹å‚è€ƒ |

---

## ğŸŒ æœåŠ¡åœ°å€

| æœåŠ¡ | é»˜è®¤åœ°å€ | è¯´æ˜ |
|------|---------|------|
| Ollama | http://localhost:11434 | Ollama æœåŠ¡ç«¯å£ |
| vLLM | http://localhost:8000 | vLLM OpenAI å…¼å®¹ç«¯å£ |
| Docker è®¿é—®ä¸»æœº | http://host.docker.internal | Docker å®¹å™¨å†…è®¿é—®ä¸»æœºæœåŠ¡ |

---

## âš¡ æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | Ollama | vLLM |
|------|--------|------|
| å¯åŠ¨é€Ÿåº¦ | â­â­â­â­â­ | â­â­â­ |
| æ¨ç†é€Ÿåº¦ | â­â­â­ | â­â­â­â­â­ |
| å¹¶å‘èƒ½åŠ› | â­â­ | â­â­â­â­â­ |
| æ˜“ç”¨æ€§ | â­â­â­â­â­ | â­â­â­ |
| æ˜¾å­˜ä¼˜åŒ– | â­â­â­â­ | â­â­â­ |

---

## ğŸ› å¸¸è§é—®é¢˜é€ŸæŸ¥

### é—®é¢˜ï¼šOllama æœåŠ¡æœªè¿è¡Œ
```bash
# è§£å†³æ–¹æ¡ˆ
ollama serve
```

### é—®é¢˜ï¼šæ¨¡å‹ä¸‹è½½é€Ÿåº¦æ…¢
```bash
# é…ç½®é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

### é—®é¢˜ï¼šDocker æ— æ³•è¿æ¥æœ¬åœ°æœåŠ¡
```yaml
# ä¿®æ”¹ models.yaml ä¸­çš„ base_url
base_url: http://host.docker.internal:11434/v1
```

### é—®é¢˜ï¼šæ˜¾å­˜ä¸è¶³
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull qwen2.5:7b  # è€Œä¸æ˜¯ 14b

# vLLM å¯ç”¨é‡åŒ–
--quantization awq
```

### é—®é¢˜ï¼šPowerShell è„šæœ¬æ— æ³•è¿è¡Œ
```powershell
# è®¾ç½®æ‰§è¡Œç­–ç•¥
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

---

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- **å®Œæ•´æŒ‡å—**: `docs/æœ¬åœ°å¤§æ¨¡å‹é…ç½®æŒ‡å—.md`
- **è„šæœ¬è¯´æ˜**: `scripts/README.md`
- **é…ç½®ç¤ºä¾‹**: `examples/config-local-models.yaml`

---

## ğŸ”— æœ‰ç”¨çš„é“¾æ¥

- [Ollama å®˜ç½‘](https://ollama.com)
- [Ollama æ¨¡å‹åº“](https://ollama.com/library)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [Qwen æ¨¡å‹](https://huggingface.co/Qwen)
- [HF é•œåƒç«™](https://hf-mirror.com)

---

**æç¤º**: å°†æ­¤æ–‡ä»¶ä¿å­˜ä¸ºä¹¦ç­¾ï¼Œæ–¹ä¾¿éšæ—¶æŸ¥é˜…ï¼
