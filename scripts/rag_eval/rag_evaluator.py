"""
RAG Evaluator - ä¸»è¯„ä¼°å¼•æ“

æ•´åˆé—®é¢˜ç”Ÿæˆã€RAG æŸ¥è¯¢ã€LLM è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆçš„å®Œæ•´æµç¨‹
"""

import asyncio
import os
import yaml
from dataclasses import dataclass
from pathlib import Path
from typing import Any

from .llm_judge import LLMJudge
from .metrics import MetricsCalculator, EvaluationSummary
from .question_generator import QuestionGenerator, TestDataset, QuestionAnswer
from .rag_executor import RAGExecutor, RAGResult
from .report_generator import ReportGenerator


@dataclass
class EvalConfig:
    """è¯„ä¼°é…ç½®"""
    # çŸ¥è¯†åº“é…ç½®
    db_id: str
    api_base_url: str = "http://localhost:5050"
    query_mode: str = "mix"
    top_k: int = 10
    
    # LLM é…ç½®
    llm_base_url: str = "http://localhost:8000/v1"
    llm_model: str = "Qwen/Qwen3-32B"
    
    # å¹¶å‘é…ç½®
    query_concurrency: int = 8
    eval_concurrency: int = 4
    
    # è®¤è¯
    username: str = ""
    password: str = ""
    
    @classmethod
    def from_yaml(cls, path: str) -> "EvalConfig":
        """ä» YAML æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        
        kb = data.get("knowledge_base", {})
        llm = data.get("evaluator_llm", {})
        conc = data.get("concurrency", {})
        
        return cls(
            db_id=kb.get("db_id", ""),
            api_base_url=data.get("api_base_url", "http://localhost:5050"),
            query_mode=kb.get("query_mode", "mix"),
            top_k=kb.get("top_k", 10),
            llm_base_url=llm.get("base_url", "http://localhost:8000/v1"),
            llm_model=llm.get("model", "Qwen/Qwen3-32B"),
            query_concurrency=conc.get("max_concurrent_queries", 8),
            eval_concurrency=conc.get("max_concurrent_evals", 4),
            username=data.get("auth", {}).get("username", ""),
            password=data.get("auth", {}).get("password", ""),
        )


class RAGEvaluator:
    """RAG è¯„ä¼°å¼•æ“"""

    def __init__(self, config: EvalConfig):
        self.config = config
        self.rag_executor = RAGExecutor(
            api_base_url=config.api_base_url,
            db_id=config.db_id,
            query_mode=config.query_mode,
            top_k=config.top_k,
        )
        self.llm_judge = LLMJudge(
            base_url=config.llm_base_url,
            model=config.llm_model,
        )
        self.metrics_calc = MetricsCalculator()
        self.report_gen = ReportGenerator()

    async def authenticate(self):
        """è®¤è¯ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if self.config.username and self.config.password:
            await self.rag_executor.login(
                self.config.username, 
                self.config.password
            )
            print(f"âœ… è®¤è¯æˆåŠŸ")

    async def run_evaluation(
        self,
        testset: TestDataset,
        progress_callback: callable = None,
    ) -> EvaluationSummary:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹
        
        Args:
            testset: æµ‹è¯•æ•°æ®é›†
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, message)
        
        Returns:
            è¯„ä¼°ç»“æœæ±‡æ€»
        """
        total = len(testset)
        print(f"å¼€å§‹è¯„ä¼° {total} ä¸ªæµ‹è¯•æ ·æœ¬...")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        eval_semaphore = asyncio.Semaphore(self.config.eval_concurrency)
        
        async def evaluate_single(idx: int, qa: QuestionAnswer):
            """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
            async with eval_semaphore:
                try:
                    # 1. æ‰§è¡Œ RAG æŸ¥è¯¢
                    rag_result = await self.rag_executor.query(qa.question)
                    
                    # 2. ä½¿ç”¨ LLM è¯„ä¼°
                    eval_results = await self.llm_judge.evaluate_all(
                        question=qa.question,
                        context=rag_result.context,
                        answer=qa.answer,  # ground truth ä½œä¸ºå‚è€ƒ
                    )
                    
                    # 3. æå–åˆ†æ•°
                    scores = {r.metric: r.score for r in eval_results}
                    
                    # 4. æ·»åŠ åˆ°è®¡ç®—å™¨
                    self.metrics_calc.add_result(
                        question=qa.question,
                        context=rag_result.context,
                        answer=qa.answer,
                        ground_truth=qa.answer,
                        faithfulness_score=scores.get("faithfulness", 0),
                        relevancy_score=scores.get("answer_relevancy", 0),
                        precision_score=scores.get("context_precision", 0),
                        latency_ms=rag_result.latency_ms,
                    )
                    
                    if progress_callback:
                        progress_callback(idx + 1, total, f"è¯„ä¼°ä¸­: {qa.question[:30]}...")
                    
                    return True
                    
                except Exception as e:
                    print(f"âŒ è¯„ä¼°æ ·æœ¬å¤±è´¥: {qa.question[:50]}... - {e}")
                    return False
        
        # å¹¶å‘è¯„ä¼°æ‰€æœ‰æ ·æœ¬
        tasks = [
            evaluate_single(idx, qa) 
            for idx, qa in enumerate(testset.items)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡æˆåŠŸç‡
        success_count = sum(1 for r in results if r is True)
        print(f"âœ… è¯„ä¼°å®Œæˆ: {success_count}/{total} æˆåŠŸ")
        
        # è®¡ç®—æ±‡æ€»
        return self.metrics_calc.calculate_summary()

    async def generate_report(
        self,
        summary: EvaluationSummary,
        output_path: str | None = None,
    ) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        # è·å–ä½åˆ†æ ·æœ¬
        low_score_samples = {}
        for metric in ["faithfulness", "answer_relevancy", "context_precision"]:
            low_score_samples[metric] = self.metrics_calc.get_low_score_samples(
                metric, threshold=0.5, limit=5
            )
        
        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        report_path = self.report_gen.save_report(
            summary, 
            low_score_samples,
            filename=output_path,
        )
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path

    async def close(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        await self.rag_executor.close()
        await self.llm_judge.close()


async def run_full_evaluation(
    docs_path: str | None = None,
    testset_path: str | None = None,
    config_path: str | None = None,
    db_id: str | None = None,
    output_dir: str = "./eval_results",
    sample_size: int | None = None,
    questions_per_doc: int = 3,
    llm_url: str | None = None,
    llm_model: str | None = None,
    username: str | None = None,
    password: str | None = None,
    concurrency: int | None = None,
) -> str:
    """
    è¿è¡Œå®Œæ•´çš„ RAG è¯„ä¼°æµç¨‹
    
    Args:
        docs_path: æ–‡æ¡£ç›®å½•ï¼ˆç”¨äºç”Ÿæˆæµ‹è¯•é—®é¢˜ï¼‰
        testset_path: å·²æœ‰çš„æµ‹è¯•é›†è·¯å¾„ï¼ˆäºŒé€‰ä¸€ï¼‰
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        db_id: çŸ¥è¯†åº“ ID
        output_dir: è¾“å‡ºç›®å½•
        sample_size: é‡‡æ ·æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        questions_per_doc: æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„é—®é¢˜æ•°
        llm_url: LLM æœåŠ¡ URLï¼ˆåœ¨çº¿ API æˆ–æœ¬åœ°æœåŠ¡ï¼‰
        llm_model: LLM æ¨¡å‹åç§°
        username: API ç”¨æˆ·å
        password: API å¯†ç 
        concurrency: LLM è¯„ä¼°å¹¶å‘æ•°
    
    Returns:
        æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½é…ç½®
    if config_path and os.path.exists(config_path):
        config = EvalConfig.from_yaml(config_path)
    else:
        config = EvalConfig(db_id=db_id or "")
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if db_id:
        config.db_id = db_id
    if llm_url:
        config.llm_base_url = llm_url
    if llm_model:
        config.llm_model = llm_model
    if username:
        config.username = username
    if password:
        config.password = password
    if concurrency:
        config.eval_concurrency = concurrency
    
    if not config.db_id:
        raise ValueError("å¿…é¡»æŒ‡å®šçŸ¥è¯†åº“ ID (db_id)")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RAGEvaluator(config)
    
    try:
        # è®¤è¯
        await evaluator.authenticate()
        
        # è·å–æµ‹è¯•é›†
        if testset_path and os.path.exists(testset_path):
            print(f"åŠ è½½æµ‹è¯•é›†: {testset_path}")
            testset = TestDataset.load(testset_path)
        elif docs_path:
            print(f"ä»æ–‡æ¡£ç”Ÿæˆæµ‹è¯•é—®é¢˜: {docs_path}")
            generator = QuestionGenerator(
                base_url=config.llm_base_url,
                model=config.llm_model,
                questions_per_doc=questions_per_doc,
            )
            testset = await generator.generate_from_directory(
                docs_path,
                max_docs=sample_size,
                concurrency=config.eval_concurrency,
            )
            await generator.close()
            
            # ä¿å­˜ç”Ÿæˆçš„æµ‹è¯•é›†
            testset_save_path = os.path.join(output_dir, "testset.jsonl")
            os.makedirs(output_dir, exist_ok=True)
            testset.save(testset_save_path)
            print(f"æµ‹è¯•é›†å·²ä¿å­˜: {testset_save_path}")
        else:
            raise ValueError("å¿…é¡»æŒ‡å®š docs_path æˆ– testset_path")
        
        # å¦‚æœæŒ‡å®šäº†é‡‡æ ·
        if sample_size and len(testset) > sample_size:
            testset.items = testset.items[:sample_size]
            print(f"é‡‡æ · {sample_size} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # è¿è¡Œè¯„ä¼°
        summary = await evaluator.run_evaluation(testset)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = await evaluator.generate_report(summary)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 50)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("=" * 50)
        for name, score in summary.metrics.items():
            print(f"  {name}: {score.mean:.2%} (Â±{score.std:.2%})")
        print(f"  å¹³å‡å»¶è¿Ÿ: {summary.avg_latency_ms:.2f}ms")
        print("=" * 50)
        
        return report_path
        
    finally:
        await evaluator.close()
